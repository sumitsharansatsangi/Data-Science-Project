{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "SRGNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6IC1JFyyJ1D",
        "colab_type": "text"
      },
      "source": [
        "### importing Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFft5jWeyJ1H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import argparse\n",
        "import time\n",
        "import csv\n",
        "import pickle\n",
        "import operator\n",
        "import datetime\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaPLKXZyyJ1U",
        "colab_type": "text"
      },
      "source": [
        "### Preprocesssing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2zIt-UoyJ1V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(dataset=\"sample\"):\n",
        "    filename = 'sample_train-item-views.csv'\n",
        "    if dataset == 'diginetica':\n",
        "        filename = './dataset-train-diginetica/train-item-views.csv'\n",
        "    elif dataset =='yoochoose':\n",
        "        filename = './yoochoose-data/yoochoose-clicks.dat'\n",
        "    print(\"-- Starting @ %ss\" % datetime.datetime.now())    \n",
        "    with open(filename, \"r\") as f:\n",
        "        if dataset == 'yoochoose':\n",
        "            reader = csv.DictReader(f, delimiter=',')\n",
        "        else:\n",
        "            reader = csv.DictReader(f, delimiter=';')\n",
        "        sess_clicks = {}\n",
        "        sess_date = {}\n",
        "        ctr = 0\n",
        "        curid = -1\n",
        "        curdate = None\n",
        "        for data in reader:\n",
        "            sessid = data['session_id']\n",
        "            if curdate and not curid == sessid:\n",
        "                date = ''\n",
        "                if dataset == 'yoochoose':\n",
        "                    date = time.mktime(time.strptime(curdate[:19], '%Y-%m-%dT%H:%M:%S'))\n",
        "                else:\n",
        "                    date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
        "                sess_date[curid] = date\n",
        "            curid = sessid\n",
        "            if dataset == 'yoochoose':\n",
        "                item = data['item_id']\n",
        "            else:\n",
        "                item = data['item_id'], int(data['timeframe'])\n",
        "            curdate = ''\n",
        "            if dataset == 'yoochoose':\n",
        "                curdate = data['timestamp']\n",
        "            else:\n",
        "                curdate = data['eventdate']\n",
        "\n",
        "            if sessid in sess_clicks:\n",
        "                sess_clicks[sessid] += [item]\n",
        "            else:\n",
        "                sess_clicks[sessid] = [item]\n",
        "            ctr += 1\n",
        "        date = ''\n",
        "        if dataset == 'yoochoose':\n",
        "            date = time.mktime(time.strptime(curdate[:19], '%Y-%m-%dT%H:%M:%S'))\n",
        "        else:\n",
        "            date = time.mktime(time.strptime(curdate, '%Y-%m-%d'))\n",
        "            for i in list(sess_clicks):\n",
        "                sorted_clicks = sorted(sess_clicks[i], key=operator.itemgetter(1))\n",
        "                sess_clicks[i] = [c[0] for c in sorted_clicks]\n",
        "        sess_date[curid] = date\n",
        "    print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n",
        "\n",
        "    # Filter out length 1 sessions\n",
        "    for s in list(sess_clicks):\n",
        "        if len(sess_clicks[s]) == 1:\n",
        "            del sess_clicks[s]\n",
        "            del sess_date[s]\n",
        "\n",
        "     # Count number of times each item appears\n",
        "    iid_counts = {}\n",
        "    for s in sess_clicks:\n",
        "        seq = sess_clicks[s]\n",
        "        for iid in seq:\n",
        "            if iid in iid_counts:\n",
        "                iid_counts[iid] += 1\n",
        "            else:\n",
        "                iid_counts[iid] = 1\n",
        "\n",
        "    sorted_counts = sorted(iid_counts.items(), key=operator.itemgetter(1))\n",
        "\n",
        "    length = len(sess_clicks)\n",
        "    for s in list(sess_clicks):\n",
        "        curseq = sess_clicks[s]\n",
        "        filseq = list(filter(lambda i: iid_counts[i] >= 5, curseq))\n",
        "        if len(filseq) < 2:\n",
        "            del sess_clicks[s]\n",
        "            del sess_date[s]\n",
        "        else:\n",
        "            sess_clicks[s] = filseq\n",
        "\n",
        "    # Split out test set based on dates\n",
        "    dates = list(sess_date.items())\n",
        "    maxdate = dates[0][1]\n",
        "\n",
        "    for _, date in dates:\n",
        "        if maxdate < date:\n",
        "            maxdate = date\n",
        "\n",
        "    # 7 days for test\n",
        "    splitdate = 0\n",
        "    if dataset == 'yoochoose':\n",
        "        splitdate = maxdate - 86400 * 1  # the number of seconds for a dayï¼š86400\n",
        "    else:\n",
        "        splitdate = maxdate - 86400 * 7\n",
        "\n",
        "    print('Splitting date', splitdate)      # Yoochoose: ('Split date', 1411930799.0)\n",
        "    tra_sess = filter(lambda x: x[1] < splitdate, dates)\n",
        "    tes_sess = filter(lambda x: x[1] > splitdate, dates)\n",
        "\n",
        "    # Sort sessions by date\n",
        "    tra_sess = sorted(tra_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
        "    tes_sess = sorted(tes_sess, key=operator.itemgetter(1))     # [(session_id, timestamp), (), ]\n",
        "    print(len(tra_sess))    # 186670    # 7966257\n",
        "    print(len(tes_sess))    # 15979     # 15324\n",
        "    print(tra_sess[:3])\n",
        "    print(tes_sess[:3])\n",
        "    print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n",
        "\n",
        "    # Choosing item count >=5 gives approximately the same number of items as reported in paper\n",
        "    item_dict = {}\n",
        "    # Convert training sessions to sequences and renumber items to start from 1\n",
        "    def obtian_tra():\n",
        "        train_ids = []\n",
        "        train_seqs = []\n",
        "        train_dates = []\n",
        "        item_ctr = 1\n",
        "        for s, date in tra_sess:\n",
        "            seq = sess_clicks[s]\n",
        "            outseq = []\n",
        "            for i in seq:\n",
        "                if i in item_dict:\n",
        "                    outseq += [item_dict[i]]\n",
        "                else:\n",
        "                    outseq += [item_ctr]\n",
        "                    item_dict[i] = item_ctr\n",
        "                    item_ctr += 1\n",
        "            if len(outseq) < 2:  # Doesn't occur\n",
        "                continue\n",
        "            train_ids += [s]\n",
        "            train_dates += [date]\n",
        "            train_seqs += [outseq]\n",
        "        print(item_ctr)     # 43098, 37484\n",
        "        return train_ids, train_dates, train_seqs\n",
        "\n",
        "\n",
        "    # Convert test sessions to sequences, ignoring items that do not appear in training set\n",
        "    def obtian_tes():\n",
        "        test_ids = []\n",
        "        test_seqs = []\n",
        "        test_dates = []\n",
        "        for s, date in tes_sess:\n",
        "            seq = sess_clicks[s]\n",
        "            outseq = []\n",
        "            for i in seq:\n",
        "                if i in item_dict:\n",
        "                    outseq += [item_dict[i]]\n",
        "            if len(outseq) < 2:\n",
        "                continue\n",
        "            test_ids += [s]\n",
        "            test_dates += [date]\n",
        "            test_seqs += [outseq]\n",
        "        return test_ids, test_dates, test_seqs\n",
        "\n",
        "\n",
        "    tra_ids, tra_dates, tra_seqs = obtian_tra()\n",
        "    tes_ids, tes_dates, tes_seqs = obtian_tes()\n",
        "\n",
        "\n",
        "    def process_seqs(iseqs, idates):\n",
        "        out_seqs = []\n",
        "        out_dates = []\n",
        "        labs = []\n",
        "        ids = []\n",
        "        for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n",
        "            for i in range(1, len(seq)):\n",
        "                tar = seq[-i]\n",
        "                labs += [tar]\n",
        "                out_seqs += [seq[:-i]]\n",
        "                out_dates += [date]\n",
        "                ids += [id]\n",
        "        return out_seqs, out_dates, labs, ids\n",
        "\n",
        "\n",
        "    tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n",
        "    te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n",
        "    tra = (tr_seqs, tr_labs)\n",
        "    tes = (te_seqs, te_labs)\n",
        "    print(len(tr_seqs))\n",
        "    print(len(te_seqs))\n",
        "    print(tr_seqs[:3], tr_dates[:3], tr_labs[:3])\n",
        "    print(te_seqs[:3], te_dates[:3], te_labs[:3])\n",
        "    all = 0\n",
        "\n",
        "    for seq in tra_seqs:\n",
        "        all += len(seq)\n",
        "    for seq in tes_seqs:\n",
        "        all += len(seq)\n",
        "    print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
        "    if dataset == 'diginetica':\n",
        "        if not os.path.exists('diginetica'):\n",
        "            os.makedirs('diginetica')\n",
        "        pickle.dump(tra, open('diginetica/train.txt', 'wb'))\n",
        "        pickle.dump(tes, open('diginetica/test.txt', 'wb'))\n",
        "        pickle.dump(tra_seqs, open('diginetica/all_train_seq.txt', 'wb'))\n",
        "    elif dataset == 'yoochoose':\n",
        "        if not os.path.exists('yoochoose1_4'):\n",
        "            os.makedirs('yoochoose1_4')\n",
        "        if not os.path.exists('yoochoose1_64'):\n",
        "            os.makedirs('yoochoose1_64')\n",
        "        pickle.dump(tes, open('yoochoose1_4/test.txt', 'wb'))\n",
        "        pickle.dump(tes, open('yoochoose1_64/test.txt', 'wb'))\n",
        "\n",
        "        split4, split64 = int(len(tr_seqs) / 4), int(len(tr_seqs) / 64)\n",
        "        print(len(tr_seqs[-split4:]))\n",
        "        print(len(tr_seqs[-split64:]))\n",
        "\n",
        "        tra4, tra64 = (tr_seqs[-split4:], tr_labs[-split4:]), (tr_seqs[-split64:], tr_labs[-split64:])\n",
        "        seq4, seq64 = tra_seqs[tr_ids[-split4]:], tra_seqs[tr_ids[-split64]:]\n",
        "\n",
        "        pickle.dump(tra4, open('yoochoose1_4/train.txt', 'wb'))\n",
        "        pickle.dump(seq4, open('yoochoose1_4/all_train_seq.txt', 'wb'))\n",
        "\n",
        "        pickle.dump(tra64, open('yoochoose1_64/train.txt', 'wb'))\n",
        "        pickle.dump(seq64, open('yoochoose1_64/all_train_seq.txt', 'wb'))\n",
        "\n",
        "    else:\n",
        "        if not os.path.exists('sample'):\n",
        "            os.makedirs('sample')\n",
        "        pickle.dump(tra, open('sample/train.txt', 'wb'))\n",
        "        pickle.dump(tes, open('sample/test.txt', 'wb'))\n",
        "        pickle.dump(tra_seqs, open('sample/all_train_seq.txt', 'wb'))\n",
        "\n",
        "    print('Done.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SmjvP-XyJ1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GNN(Module):\n",
        "    def __init__(self, hidden_size, step=1):\n",
        "        super(GNN, self).__init__()\n",
        "        self.step = step\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = hidden_size * 2\n",
        "        self.gate_size = 3 * hidden_size\n",
        "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
        "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
        "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
        "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
        "\n",
        "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "\n",
        "    def GNNCell(self, A, hidden):\n",
        "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
        "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n",
        "        inputs = torch.cat([input_in, input_out], 2)\n",
        "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
        "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
        "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
        "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
        "        resetgate = torch.sigmoid(i_r + h_r)\n",
        "        inputgate = torch.sigmoid(i_i + h_i)\n",
        "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
        "        hy = newgate + inputgate * (hidden - newgate)\n",
        "        return hy\n",
        "\n",
        "    def forward(self, A, hidden):\n",
        "        for i in range(self.step):\n",
        "            hidden = self.GNNCell(A, hidden)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class SessionGraph(Module):\n",
        "    def __init__(self, param, n_node):\n",
        "        super(SessionGraph, self).__init__()\n",
        "        self.hidden_size = param[\"hiddenSize\"]\n",
        "        self.n_node = n_node\n",
        "        self.batch_size = param[\"batchSize\"]\n",
        "        self.nonhybrid = param[\"onhybrid\"]\n",
        "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
        "        self.gnn = GNN(self.hidden_size, step=param[\"step\"])\n",
        "        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=param[\"lr\"], weight_decay=param[\"l2\"])\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=param[\"lr_dc_step\"], gamma=param[\"lr_dc\"])\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def compute_scores(self, hidden, mask):\n",
        "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]  # batch_size x latent_size\n",
        "        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  # batch_size x 1 x latent_size\n",
        "        q2 = self.linear_two(hidden)  # batch_size x seq_length x latent_size\n",
        "        alpha = self.linear_three(torch.sigmoid(q1 + q2))\n",
        "        a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)\n",
        "        if not self.nonhybrid:\n",
        "            a = self.linear_transform(torch.cat([a, ht], 1))\n",
        "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
        "        scores = torch.matmul(a, b.transpose(1, 0))\n",
        "        return scores\n",
        "\n",
        "    def forward(self, inputs, A):\n",
        "        hidden = self.embedding(inputs)\n",
        "        hidden = self.gnn(A, hidden)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "def trans_to_cuda(variable):\n",
        "    if torch.cuda.is_available():\n",
        "        return variable.cuda()\n",
        "    else:\n",
        "        return variable\n",
        "\n",
        "\n",
        "def trans_to_cpu(variable):\n",
        "    if torch.cuda.is_available():\n",
        "        return variable.cpu()\n",
        "    else:\n",
        "        return variable\n",
        "\n",
        "\n",
        "def forward(model, i, data):\n",
        "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
        "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
        "    items = trans_to_cuda(torch.Tensor(items).long())\n",
        "    A = trans_to_cuda(torch.Tensor(A).float())\n",
        "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
        "    hidden = model(items, A)\n",
        "    get = lambda i: hidden[i][alias_inputs[i]]\n",
        "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
        "    return targets, model.compute_scores(seq_hidden, mask)\n",
        "\n",
        "\n",
        "def train_test(model, train_data, test_data):\n",
        "    model.scheduler.step()\n",
        "    print('start training: ', datetime.datetime.now())\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    slices = train_data.generate_batch(model.batch_size)\n",
        "    for i, j in zip(slices, np.arange(len(slices))):\n",
        "        model.optimizer.zero_grad()\n",
        "        targets, scores = forward(model, i, train_data)\n",
        "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
        "        loss = model.loss_function(scores, targets - 1)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        total_loss += loss\n",
        "        if j % int(len(slices) / 5 + 1) == 0:\n",
        "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
        "    print('\\tLoss:\\t%.3f' % total_loss)\n",
        "\n",
        "    print('start predicting: ', datetime.datetime.now())\n",
        "    model.eval()\n",
        "    hit, mrr = [], []\n",
        "    slices = test_data.generate_batch(model.batch_size)\n",
        "    for i in slices:\n",
        "        targets, scores = forward(model, i, test_data)\n",
        "        sub_scores = scores.topk(20)[1]\n",
        "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
        "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
        "            hit.append(np.isin(target - 1, score))\n",
        "            if len(np.where(score == target - 1)[0]) == 0:\n",
        "                mrr.append(0)\n",
        "            else:\n",
        "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
        "    hit = np.mean(hit) * 100\n",
        "    mrr = np.mean(mrr) * 100\n",
        "    return hit, mrr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4xd_v_QyJ1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_graph(train_data):\n",
        "    graph = nx.DiGraph()\n",
        "    for seq in train_data:\n",
        "        for i in range(len(seq) - 1):\n",
        "            if graph.get_edge_data(seq[i], seq[i + 1]) is None:\n",
        "                weight = 1\n",
        "            else:\n",
        "                weight = graph.get_edge_data(seq[i], seq[i + 1])['weight'] + 1\n",
        "            graph.add_edge(seq[i], seq[i + 1], weight=weight)\n",
        "    for node in graph.nodes:\n",
        "        sum = 0\n",
        "        for j, i in graph.in_edges(node):\n",
        "            sum += graph.get_edge_data(j, i)['weight']\n",
        "        if sum != 0:\n",
        "            for j, i in graph.in_edges(i):\n",
        "                graph.add_edge(j, i, weight=graph.get_edge_data(j, i)['weight'] / sum)\n",
        "    return graph\n",
        "\n",
        "\n",
        "def data_masks(all_usr_pois, item_tail):\n",
        "    us_lens = [len(upois) for upois in all_usr_pois]\n",
        "    len_max = max(us_lens)\n",
        "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
        "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
        "    return us_pois, us_msks, len_max\n",
        "\n",
        "\n",
        "def split_validation(train_set, valid_portion):\n",
        "    train_set_x, train_set_y = train_set\n",
        "    n_samples = len(train_set_x)\n",
        "    sidx = np.arange(n_samples, dtype='int32')\n",
        "    np.random.shuffle(sidx)\n",
        "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
        "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
        "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
        "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
        "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
        "\n",
        "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)\n",
        "\n",
        "\n",
        "class Data():\n",
        "    def __init__(self, data, shuffle=False, graph=None):\n",
        "        inputs = data[0]\n",
        "        inputs, mask, len_max = data_masks(inputs, [0])\n",
        "        self.inputs = np.asarray(inputs)\n",
        "        self.mask = np.asarray(mask)\n",
        "        self.len_max = len_max\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(inputs)\n",
        "        self.shuffle = shuffle\n",
        "        self.graph = graph\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.inputs = self.inputs[shuffled_arg]\n",
        "            self.mask = self.mask[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, i):\n",
        "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
        "        items, n_node, A, alias_inputs = [], [], [], []\n",
        "        for u_input in inputs:\n",
        "            n_node.append(len(np.unique(u_input)))\n",
        "        max_n_node = np.max(n_node)\n",
        "        for u_input in inputs:\n",
        "            node = np.unique(u_input)\n",
        "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "            u_A = np.zeros((max_n_node, max_n_node))\n",
        "            for i in np.arange(len(u_input) - 1):\n",
        "                if u_input[i + 1] == 0:\n",
        "                    break\n",
        "                u = np.where(node == u_input[i])[0][0]\n",
        "                v = np.where(node == u_input[i + 1])[0][0]\n",
        "                u_A[u][v] = 1\n",
        "            u_sum_in = np.sum(u_A, 0)\n",
        "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "            u_A_in = np.divide(u_A, u_sum_in)\n",
        "            u_sum_out = np.sum(u_A, 1)\n",
        "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
        "            A.append(u_A)\n",
        "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "        return alias_inputs, A, items, mask, targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52QOAHaIyJ1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SR_GNN(dataset='sample',batchSize=100 ,hiddenSize=100,epoch=30,lr=0.001,lr_dc=0.,lr_dc_step=3          \n",
        ",l2=1e-5,step=1,patience=10,onhybrid=True,validation=True,valid_portion=0.1):\n",
        "\n",
        "    param={}\n",
        "    param[\"dataset\"]=dataset\n",
        "    param[\"batchSize\"]=batchSize\n",
        "    param[\"hiddenSize\"]=hiddenSize\n",
        "    param[\"epoch\"]=epoch\n",
        "    param[\"lr\"]=lr\n",
        "    param[\"lr_dc\"]=lr_dc\n",
        "    param[\"lr_dc_step\"]=lr_dc_step\n",
        "    param[\"l2\"]=l2\n",
        "    param[\"step\"]=step\n",
        "    param[\"patience\"]=patience\n",
        "    param[\"onhybrid\"]=onhybrid\n",
        "    param[\"validation\"]=validation\n",
        "    param[\"valid_portion\"]=valid_portion\n",
        "    train_data = pickle.load(open('./'+dataset + '/train.txt', 'rb'))\n",
        "    if validation:\n",
        "        train_data, valid_data = split_validation(train_data,valid_portion)\n",
        "        test_data = valid_data\n",
        "    else:\n",
        "        test_data = pickle.load(open('../datasets/' + dataset + '/test.txt', 'rb'))\n",
        "    # all_train_seq = pickle.load(open('../datasets/' + opt.dataset + '/all_train_seq.txt', 'rb'))\n",
        "    # g = build_graph(all_train_seq)\n",
        "    train_data = Data(train_data, shuffle=True)\n",
        "    test_data = Data(test_data, shuffle=False)\n",
        "    # del all_train_seq, g\n",
        "    if dataset == 'diginetica':\n",
        "        n_node = 43098\n",
        "    elif dataset == 'yoochoose1_64' or dataset == 'yoochoose1_4':\n",
        "        n_node = 37484\n",
        "    else:\n",
        "        n_node = 310\n",
        "\n",
        "    model = trans_to_cuda(SessionGraph(param, n_node))\n",
        "\n",
        "    start = time.time()\n",
        "    best_result = [0, 0]\n",
        "    best_epoch = [0, 0]\n",
        "    bad_counter = 0\n",
        "    for epoch in range(param[\"epoch\"]):\n",
        "        print('-------------------------------------------------------')\n",
        "        print('epoch: ', epoch)\n",
        "        hit, mrr = train_test(model, train_data, test_data)\n",
        "        flag = 0\n",
        "        if hit >= best_result[0]:\n",
        "            best_result[0] = hit\n",
        "            best_epoch[0] = epoch\n",
        "            flag = 1\n",
        "        if mrr >= best_result[1]:\n",
        "            best_result[1] = mrr\n",
        "            best_epoch[1] = epoch\n",
        "            flag = 1\n",
        "        print('Best Result:')\n",
        "        print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d'% (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
        "        bad_counter += 1 - flag\n",
        "        if bad_counter >= param[\"patience\"]:\n",
        "            break\n",
        "    print('-------------------------------------------------------')\n",
        "    end = time.time()\n",
        "    print(\"Run time: %f s\" % (end - start))\n",
        "    plt.plot\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8c6907eb-c673-4996-a8dd-4d9b95e39f4e",
        "id": "md8thS9E3z-D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "path = './code/SR-GNN-master/datasets'\n",
        " \n",
        "files = os.listdir(path)\n",
        "for name in files:\n",
        "    print(name)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataset-train-diginetica\n",
            "yoochoose-data\n",
            "preprocess.py\n",
            "sample\n",
            "tasty.csv\n",
            "diginetica\n",
            "sample_train-item-views.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rm_V34piss7g",
        "colab_type": "code",
        "outputId": "c518e63a-35cd-49dc-ef1f-184c1bc6f0a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.chdir(path)\n",
        "preprocessing()\n",
        "SR_GNN()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-- Starting @ 2019-11-06 03:16:49.969296s\n",
            "-- Reading data @ 2019-11-06 03:16:50.290404s\n",
            "Splitting date 1464134400.0\n",
            "469\n",
            "47\n",
            "[('2671', 1451952000.0), ('1211', 1452384000.0), ('3780', 1452384000.0)]\n",
            "[('1864', 1464220800.0), ('1867', 1464220800.0), ('1868', 1464220800.0)]\n",
            "-- Splitting train set and test set @ 2019-11-06 03:16:50.302245s\n",
            "310\n",
            "1205\n",
            "99\n",
            "[[1, 2], [1], [4]] [1451952000.0, 1451952000.0, 1452384000.0] [3, 2, 5]\n",
            "[[282], [281, 308], [281]] [1464220800.0, 1464220800.0, 1464220800.0] [282, 281, 308]\n",
            "avg length:  3.5669291338582676\n",
            "Done.\n",
            "-------------------------------------------------------\n",
            "epoch:  0\n",
            "start training:  2019-11-06 03:16:51.101327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0/11] Loss: 5.7555\n",
            "[3/11] Loss: 5.7213\n",
            "[6/11] Loss: 5.6675\n",
            "[9/11] Loss: 5.6276\n",
            "\tLoss:\t62.578\n",
            "start predicting:  2019-11-06 03:16:52.015838\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  1\n",
            "start training:  2019-11-06 03:16:52.067061\n",
            "[0/11] Loss: 5.5724\n",
            "[3/11] Loss: 5.4866\n",
            "[6/11] Loss: 5.4375\n",
            "[9/11] Loss: 5.2832\n",
            "\tLoss:\t59.661\n",
            "start predicting:  2019-11-06 03:16:52.650035\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  2\n",
            "start training:  2019-11-06 03:16:52.699100\n",
            "[0/11] Loss: 5.1694\n",
            "[3/11] Loss: 5.2855\n",
            "[6/11] Loss: 5.3208\n",
            "[9/11] Loss: 5.1034\n",
            "\tLoss:\t56.630\n",
            "start predicting:  2019-11-06 03:16:53.272009\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  3\n",
            "start training:  2019-11-06 03:16:53.313284\n",
            "[0/11] Loss: 5.1481\n",
            "[3/11] Loss: 5.2510\n",
            "[6/11] Loss: 4.9598\n",
            "[9/11] Loss: 5.0006\n",
            "\tLoss:\t56.669\n",
            "start predicting:  2019-11-06 03:16:53.888233\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  4\n",
            "start training:  2019-11-06 03:16:53.929575\n",
            "[0/11] Loss: 5.1726\n",
            "[3/11] Loss: 5.0142\n",
            "[6/11] Loss: 5.1186\n",
            "[9/11] Loss: 5.1409\n",
            "\tLoss:\t56.680\n",
            "start predicting:  2019-11-06 03:16:54.498003\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  5\n",
            "start training:  2019-11-06 03:16:54.540226\n",
            "[0/11] Loss: 5.0820\n",
            "[3/11] Loss: 5.1911\n",
            "[6/11] Loss: 5.0270\n",
            "[9/11] Loss: 5.0447\n",
            "\tLoss:\t56.682\n",
            "start predicting:  2019-11-06 03:16:55.129472\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  6\n",
            "start training:  2019-11-06 03:16:55.177381\n",
            "[0/11] Loss: 5.1773\n",
            "[3/11] Loss: 5.1733\n",
            "[6/11] Loss: 5.1189\n",
            "[9/11] Loss: 5.1503\n",
            "\tLoss:\t56.672\n",
            "start predicting:  2019-11-06 03:16:55.742638\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  7\n",
            "start training:  2019-11-06 03:16:55.783929\n",
            "[0/11] Loss: 5.1475\n",
            "[3/11] Loss: 5.1780\n",
            "[6/11] Loss: 5.2187\n",
            "[9/11] Loss: 5.2255\n",
            "\tLoss:\t56.673\n",
            "start predicting:  2019-11-06 03:16:56.351651\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  8\n",
            "start training:  2019-11-06 03:16:56.393396\n",
            "[0/11] Loss: 5.2901\n",
            "[3/11] Loss: 5.1364\n",
            "[6/11] Loss: 5.2602\n",
            "[9/11] Loss: 5.1800\n",
            "\tLoss:\t56.664\n",
            "start predicting:  2019-11-06 03:16:56.940294\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  9\n",
            "start training:  2019-11-06 03:16:56.983218\n",
            "[0/11] Loss: 5.1382\n",
            "[3/11] Loss: 5.0196\n",
            "[6/11] Loss: 5.0730\n",
            "[9/11] Loss: 5.0540\n",
            "\tLoss:\t56.676\n",
            "start predicting:  2019-11-06 03:16:57.553452\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "epoch:  10\n",
            "start training:  2019-11-06 03:16:57.595389\n",
            "[0/11] Loss: 5.1770\n",
            "[3/11] Loss: 5.0763\n",
            "[6/11] Loss: 5.2761\n",
            "[9/11] Loss: 5.0277\n",
            "\tLoss:\t56.685\n",
            "start predicting:  2019-11-06 03:16:58.172821\n",
            "Best Result:\n",
            "\tRecall@20:\t52.0661\tMMR@20:\t32.1141\tEpoch:\t0,\t0\n",
            "-------------------------------------------------------\n",
            "Run time: 7.119674 s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}